{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "mount_file_id": "1daSJtdn_AOYUWkjqZY-aeWji_0McF8Dk",
      "authorship_tag": "ABX9TyPKLKehQIaUej/i+uavEfSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CCanxue/CCanxue/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsCAZ1HuMXq",
        "outputId": "9508adcc-2a1c-483d-e9d5-f0a0aed89f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3., 4.], dtype=torch.float64) tensor(5., dtype=torch.float64)\n",
            "tensor(11., dtype=torch.float64)\n",
            "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)\n",
            "tensor([[0., 1., 2., 3.],\n",
            "        [4., 5., 6., 7.]], dtype=torch.float64)\n",
            "[1 2 3 4]\n",
            "12\n",
            "tensor([[9., 9., 9., 9.],\n",
            "        [9., 9., 9., 9.],\n",
            "        [9., 9., 9., 9.]], dtype=torch.float64)\n",
            "140102917043888\n",
            "140102916823760\n",
            "tensor([[108., 108., 108., 108., 108.]], dtype=torch.float64)\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "#预备知识练习\n",
        "#id() dir() help() x[a:b] 从a到b，不包括b  len(x) axis0上元素个数 矩阵运算用：x+=y x*=y  torch.cat() 矩阵拼接\n",
        "#A.mean(axis=0)=A.sum(axis=0)/A.shape[0]  torch.matmul() 矩阵/向量乘积均适用\n",
        "import torch\n",
        "import numpy as np\n",
        "z=np.ones((4,5))\n",
        "z=torch.tensor(z)\n",
        "x=torch.arange(12,dtype=torch.float64)\n",
        "print(x[1:5],x[5])  \n",
        "print(x[-1])   #最后一位\n",
        "x=x.reshape((3,-1))\n",
        "print(x[-1]) \n",
        "print(x[0:-1])  \n",
        "x[0:-1]=9  #x中除最后一行的其他元素，不管x是几维\n",
        "x[:]=9   #x中的全部元素\n",
        "y=torch.tensor([1,2,3,4])\n",
        "y=y.numpy()\n",
        "print(y)\n",
        "print(x.numel())  #全部元素的个数\n",
        "print(x)\n",
        "print(id(x))\n",
        "x=torch.mm(x,z)   #矩阵乘法  dot为向量点积\n",
        "print(id(x))\n",
        "print(x.sum(axis=0,keepdims=True))\n",
        "print(len(z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#计算梯度\n",
        "import torch\n",
        "x=torch.arange(4,dtype=torch.float32,requires_grad=True)\n",
        "x=x.reshape(2,2)\n",
        "x.retain_grad()\n",
        "y=torch.matmul(x,x)*2+6\n",
        "y.backward(torch.ones_like(x))\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "Z7C_-U5cyMIl",
        "outputId": "9e10afbf-055e-43ec-86d0-133166b607df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6., 14.],\n",
            "        [10., 18.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "def synthetic_data(w,b,num_examples):\n",
        "  x=torch.normal(0,1,(num_examples,len(w)))\n",
        "  y=torch.matmul(x,w)+b   \n",
        "  y+=torch.normal(0,0.01,y.shape) #节省内存\n",
        "  return x,y.reshape((-1,1))\n",
        "true_w=torch.tensor([2.,-3.4])\n",
        "true_b=4.2\n",
        "features,labels=synthetic_data(true_w,true_b,1000)\n",
        "def data_iter(batch_size,features,labels):\n",
        "  num_examples=len(features)\n",
        "  indices=list(range(num_examples))\n",
        "  random.shuffle(indices) #洗牌\n",
        "  for i in range(0,num_examples,batch_size):\n",
        "    batch_indices=torch.tensor(indices[i:min(i+batch_size,num_examples)])\n",
        "    yield features[batch_indices],labels[batch_indices]\n",
        "batch_size=10\n",
        "w=torch.normal(0,0.01,(2,1),requires_grad=True)\n",
        "b=torch.zeros(1,requires_grad=True)\n",
        "def linreg(x,w,b):\n",
        "  return torch.matmul(x,w)+b\n",
        "def squared_loss(y_hat,y):\n",
        "  return (y_hat-y.reshape(y_hat.shape))**2/2\n",
        "def sgd(params,lr,batch_size):\n",
        "  with torch.no_grad():\n",
        "    for param in params:\n",
        "      param-=lr*param.grad/batch_size\n",
        "      param.grad.zero_() #梯度更新\n",
        "lr=0.03\n",
        "num_epochs=3\n",
        "net=linreg\n",
        "loss=squared_loss\n",
        "for epoch in range(num_epochs):\n",
        "  for x,y in data_iter(batch_size,features,labels):\n",
        "    l=loss(net(x,w,b),y)\n",
        "    l.sum().backward()\n",
        "    sgd([w,b],lr,batch_size)\n",
        "  with torch.no_grad():\n",
        "    train_l=loss(net(features,w,b),labels)\n",
        "    print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')"
      ],
      "metadata": {
        "id": "4SbGP4_T8_wB",
        "outputId": "4216a7f5-eac5-483b-b565-679b60fe22d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch1,loss0.049463\n",
            "epoch2,loss0.000212\n",
            "epoch3,loss0.000051\n"
          ]
        }
      ]
    }
  ]
}